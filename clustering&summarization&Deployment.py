# -*- coding: utf-8 -*-
"""Clustering&Summarization__(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OxMoyg4gTT87rJmsReBrAh4wTf3oDf7u
"""

pip install python-dotenv

# ===============================
# INSTALL REQUIRED LIBRARIES
# ===============================
# !pip install -q sentence-transformers

# ===============================
# IMPORTS
# ===============================
import os
import re
import numpy as np
import pandas as pd
from collections import Counter
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import kagglehub

# ===============================
# SETUP: STOPWORDS AND STEMMER
# ===============================
nltk.download('stopwords')

# Default + custom stopwords

# ===============================
# SETUP: STOPWORDS AND STEMMER
# ===============================
nltk.download('stopwords')

# Default + custom stopwords
ignored_words = set(stopwords.words('english'))
ignored_words.update([
    # Sentiment words
    'great', 'good', 'nice', 'use','love',  'amazing', 'perfect', 'easy',
    'like', 'helpful', 'recommend', 'excellent', 'well', 'awesome',
    'best', 'worst', 'fantastic', 'disappointed', 'okay' ,'loves', 'kids',

    # Generic product terms
    'product', 'item', 'thing', 'buy', 'purchase', 'price', 'deal',
    'value', 'shipping', 'order', 'deliver', 'return', 'quality',

    # Tech/common words'brand',
    'black', 'white', 'case', 'new', 'edition', 'generation',
    'certified', 'refurbished', 'inch', 'display', 'wifi', 'color',

    # Dataset-specific noisy labels
    'tabletsamazon', 'tabletstabletsal', 'tabletstabletscomput', 'tabletseread',
    'readerselectron', 'musicelectronicsipad', 'tabletsandroid', 'electronicscolleg',
    'accessorieshom', 'storeelectron', 'brandsamazon', 'collegecolleg',
])

stemmer = PorterStemmer()

# ===============================
# FUNCTION: TEXT CLEANING
# ===============================
def super_clean(text):
    if pd.isna(text):
        return ""

    text = str(text).lower()

    text = re.sub(r"[^a-zA-Z0-9]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    tokens = [w for w in text.split() if w not in ignored_words and len(w) > 2]
    return " ".join(tokens)


# ===============================
# LOAD & CLEAN DATA FROM MULTIPLE FILES
# ===============================
print("Downloading dataset...")
organizations__path = kagglehub.dataset_download('datafiniti/consumer-reviews-of-amazon-products')
print("Data source import complete.")

file_paths = [
    '/kaggle/input/consumer-reviews-of-amazon-products/1429_1.csv',
    #'/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv',
   # '/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv'
]

all_data = []

# Read, clean and combine text columns
for path in file_paths:
    df = pd.read_csv(path, dtype={"categories": str, "reviews.text": str}, low_memory=False)
    if all(col in df.columns for col in ["categories", "reviews.text"]):
        df = df[["categories", "reviews.text"]].copy()
        df["categories"] = df["categories"].apply(super_clean)
        df["reviews.text"] = df["reviews.text"].apply(super_clean)
        df["combined"] = df["reviews.text"] + " " + df["categories"]
        all_data.append(df)

# Combine all datasets into one
merged_df = pd.concat(all_data, ignore_index=True)
merged_df.dropna(subset=["combined"], inplace=True)
merged_df.drop_duplicates(subset=["combined"], inplace=True)

# ===============================
# EMBEDDINGS: LOAD OR COMPUTE
# ===============================
model = SentenceTransformer("intfloat/e5-small-v2")
text_list = [f"query: {text}" for text in merged_df["combined"]]

if os.path.exists("embeddings.npy"):
    cached = np.load("embeddings.npy")
    if len(cached) == len(text_list):
        print("‚úÖ Loaded cached embeddings")
        embeddings = cached
    else:
        print("‚ö†Ô∏è Embedding count mismatch. Recomputing...")
        embeddings = model.encode(text_list, show_progress_bar=True)
        np.save("embeddings.npy", embeddings)
else:
    print("üß† Calculating embeddings...")
    embeddings = model.encode(text_list, show_progress_bar=True)
    np.save("embeddings.npy", embeddings)

# ===============================
# APPLY KMEANS CLUSTERING
# ===============================
num_clusters = 5
kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)
cluster_labels = kmeans.fit_predict(embeddings)
merged_df["cluster"] = cluster_labels

# ===============================
# OPTIONAL: CLUSTER LABEL MAPPING
# ===============================

# ===============================
# OPTIONAL: CLUSTER LABEL MAPPING
# ===============================
merge_map = {
  0: "Digital Reading & Productivity Tools",
    1: "Smart Audio & Entertainment Devices",
    2: "Tablets & Consumer Electronics",
    3: "Streaming Devices & Media Playback"
}
merged_df['merged_cluster'] = merged_df['cluster'].map(merge_map)

# ===============================
# SHOW TOP WORDS PER CLUSTER
# ===============================
for name in merged_df['merged_cluster'].unique():
    texts = merged_df[merged_df['merged_cluster'] == name]['combined'].tolist()
    all_words = " ".join(texts).split()
    filtered = [w for w in all_words if w not in ignored_words and len(w) > 2]
    word_counts = Counter(filtered)
    common_words = word_counts.most_common(10)

    print(f"\n{name} - Top Words:")
    for word, count in common_words:
        print(f"{word} ({count})")


# Save the result
merged_df.to_csv("clustered_reviews.csv", index=False)
print("Clustered reviews saved to 'clustered_reviews.csv'")



# Show the number of records per merged category
print(merged_df['merged_cluster'].value_counts())
# Remove rows with missing merged_cluster values
merged_df.dropna(subset=["merged_cluster"], inplace=True)
# Remove duplicate rows based on merged_cluster column
merged_df.drop_duplicates(subset=["merged_cluster"], inplace=True)
# Remove rows where merged_cluster is empty or whitespace
merged_df = merged_df[merged_df["merged_cluster"].str.strip().astype(bool)]

print(df.columns.tolist())

!pip install gradio

!pip install openai

import openai
import os

# üîê Paste your API key here (replace the text between quotes)
openai.api_key = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

!pip install python-dotenv

from google.colab import files
uploaded = files.upload()

# ‚úÖ Install dependencies if not already installed
!pip install openai python-dotenv

# ‚úÖ Import libraries
import os
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv

# ‚úÖ Load API key from .env file
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# ‚úÖ Check that the key exists
if not api_key or api_key.startswith("sk-") is False:
    raise ValueError("‚ùå OPENAI_API_KEY not found or invalid. Please check your .env file.")

# ‚úÖ Initialize OpenAI client
client = OpenAI(api_key=api_key)

# ‚úÖ Load your clustered reviews file
file_path = "clustered_reviews (2).csv"  # make sure file is uploaded or in the right path
df = pd.read_csv(file_path)

# ‚úÖ Define cluster name mapping
cluster_map = {
    0: "Digital Reading & Productivity Tools",
    1: "Smart Audio & Entertainment Devices",
    2: "Tablets & Consumer Electronics",
    3: "Streaming Devices & Media Playback",
    4: "Kids & Family Tablets"
}

# ‚úÖ Clean the DataFrame
df = df.dropna(subset=["cluster", "combined"])

# ‚úÖ Create summaries directory
os.makedirs("summaries", exist_ok=True)

# ‚úÖ Function to generate summary with GPT-3.5
def summarize_with_gpt(cluster_name, reviews):
    prompt = f"""
You are an AI assistant. Based only on the customer reviews below, write a professional and well-structured article for the product cluster: "{cluster_name}".

Your article must include:
1. The top 3 products and the key differences between them.
2. The top complaints for each product.
3. The worst product and why users should avoid it.

Do not repeat any product name or sentence unnecessarily. Stay concise and accurate. Use only the content found in the reviews.

Reviews:
{reviews[:5000]}
"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=800
    )

    return response.choices[0].message.content

# ‚úÖ Generate summaries and save them
results = []

for cluster_id, cluster_name in cluster_map.items():
    cluster_reviews = df[df["cluster"] == cluster_id]["combined"].tolist()
    if not cluster_reviews:
        continue

    joined_reviews = " ".join(cluster_reviews)
    summary = summarize_with_gpt(cluster_name, joined_reviews)

    # Save each summary as .txt
    filename = f"summaries/{cluster_name.replace(' ', '_')}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(summary)

    # Store for Excel export
    results.append({
        "cluster_id": cluster_id,
        "cluster_name": cluster_name,
        "summary": summary
    })

# ‚úÖ Save all summaries to Excel
summary_df = pd.DataFrame(results)
summary_df.to_excel("all_summaries.xlsx", index=False)

# ‚úÖ Display top results
summary_df.head()

!pip install gradio openai python-dotenv

import os
import pandas as pd
import gradio as gr
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variable from .env
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# Validate API key
if not api_key or not api_key.startswith("sk-"):
    raise ValueError("‚ùå OPENAI_API_KEY not found or invalid.")

# OpenAI Client
client = OpenAI(api_key=api_key)

# Define cluster name mapping
cluster_map = {
    0: "Digital Reading & Productivity Tools",
    1: "Smart Audio & Entertainment Devices",
    2: "Tablets & Consumer Electronics",
    3: "Streaming Devices & Media Playback",
    4: "Kids & Family Tablets"
}
'''
def summarize_with_gpt(cluster_name, reviews):
    prompt = f"""
You are an expert product analyst.

Based ONLY on the customer reviews provided below, write a clear and professional article about the product cluster: "{cluster_name}".

Your article must include:

1. üèÜ **The top 3 products** (by name, if possible), with clear comparisons and key differences between them.
2. ‚ö†Ô∏è **The top complaints** for each of those 3 products.
3. ‚ùå **The worst-performing product** (by name, if it appears), and a short explanation of why it should be avoided.

‚ú≥Ô∏è Instructions:
- Use only names and details mentioned in the reviews.
- Do not invent product names.
- If multiple products are mentioned, extract the names clearly from the reviews.
- Avoid repeating the same sentence or phrase.
- Keep your language clean, organized, and user-friendly.

üìù Here are the reviews:
{reviews[:5000]}
"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=800
    )

    return response.choices[0].message.content
'''
def summarize_with_gpt(cluster_name, reviews):
    prompt = f"""
You are a professional product writer.

Based ONLY on the customer reviews below, write a clear and professional article about the product cluster: "{cluster_name}".

The article must be structured into **exactly three paragraphs**, where each paragraph should include:
- The top 3 products mentioned in the reviews (with names if available), and the key differences between them.
- The most common complaints or issues users had for each product.
- The worst product (by name if possible), and a short explanation of why it should be avoided.

Additional Guidelines:
- Do not repeat any sentence or product name unnecessarily.
- Use smooth transitions like ‚Äúfirst‚Äù, ‚Äúsecond‚Äù, and ‚Äúfinally‚Äù.
- Write in full paragraphs ‚Äî do not use bullet points or numbered lists.
- Do not say ‚Äúbased on the reviews below‚Äù ‚Äî treat this as a standalone article.
- Keep the article brief and well-structured ‚Äî ideally no more than **10 lines total**.
- Only use names and details that appear in the reviews. Do not invent anything.

Here are the reviews:
{reviews[:5000]}
"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=1000
    )

    return response.choices[0].message.content


# Extract clusters from uploaded file
def extract_cluster_names(file):
    df = pd.read_csv(file.name) if file.name.endswith(".csv") else pd.read_excel(file.name)
    df.dropna(subset=["cluster", "combined"], inplace=True)
    return gr.update(choices=list(cluster_map.values())), df

# Generate summary for selected cluster
def generate_summary(file, cluster_name):
    df = pd.read_csv(file.name) if file.name.endswith(".csv") else pd.read_excel(file.name)
    df.dropna(subset=["cluster", "combined"], inplace=True)

    cluster_id = next((k for k, v in cluster_map.items() if v == cluster_name), None)
    reviews = df[df["cluster"] == cluster_id]["combined"].tolist()

    if not reviews:
        return "‚ùå No reviews found for this cluster."

    joined_reviews = " ".join(reviews)
    summary = summarize_with_gpt(cluster_name, joined_reviews)

    # Save summary
    os.makedirs("summaries", exist_ok=True)
    with open(f"summaries/{cluster_name.replace(' ', '_')}.txt", "w", encoding="utf-8") as f:
        f.write(summary)

    return summary

# Gradio UI
with gr.Blocks(title="üß† GPT-3.5 Review Cluster Summarizer") as demo:
    gr.Markdown("## üìä Generate a summary from product review clusters")

    file_input = gr.File(label="üìÅ Upload clustered reviews CSV or Excel")
    dropdown = gr.Dropdown(label="üìå Select Cluster Name", choices=[], interactive=True)
    output = gr.Textbox(label="üìù Generated Summary", lines=18)

    file_input.change(fn=extract_cluster_names, inputs=file_input, outputs=[dropdown, gr.State()])
    dropdown.change(fn=generate_summary, inputs=[file_input, dropdown], outputs=output)

demo.launch(share=True)